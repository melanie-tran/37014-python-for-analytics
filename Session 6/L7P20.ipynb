{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing techniques generally refer to the addition, deletion, or transformation of training set data. Different models have different sensitivities to the type of predictors in the model; *how* the predictors enter the model is also important.\n",
    "\n",
    "The need for data pre-processing is determined by the type of model being used. Some procedures, such as tree-based models, are notably insensitive to the characteristics of the predictor data. Others, like linear regression, are not. \n",
    "\n",
    "How the predictors are encoded, called *feature engineering*, can have a significant impact on model performance. Often the most effective encoding of the data is informed by the modeler's understanding of the problem and thus is not derived from any mathematical techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Case Study: Cell Segmentation in High-Content Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data downloaded from \n",
    "# https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is from Hill et al. (2007) that consists of 2019 cells. Of these cells, 1300 were judged to be poorly segmented (PS) and 719 were well segmented (WS); 1009 cells were reserved for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cell_segmentation = pd.read_csv(\"segmentationOriginal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_segmentation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_segmentation.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training set samples to demonstrate data pre-processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_segmentation.groupby('Case').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data\n",
    "cell_train = cell_segmentation.loc[cell_segmentation['Case'] == 'Train']\n",
    "cell_test = cell_segmentation.loc[cell_segmentation['Case'] == 'Test']\n",
    "\n",
    "cell_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Transformation for Individual Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations of predictor variables may be needed for several reasons. Some modeling techniques may have strict requirements, such as the predictors having a commom scale. In other cases, creating a good model may be difficult due to specific characteristics of the data (e.g., outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center a predictor variable, the average predictor value is substracted from all the values. As a result of centering, the predictor has a zero mean. Similarly, to scale the data, each value of the predictor variable is divided by its standard deviation. Scaling the data coerce the values to have a common standard deviation of one. These manipulations are generally used to improve the numerical stability of some calculations. The only real downside to these transformation is a loss of interpretability of the individual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations to Resolve Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An un-skewed distribution is one that is roughly symmetric. A rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. The sample skewness statistic is defined $$\\text{skewness} = {\\sum (x_i - \\bar{x})^3 \\over (n - 1) v^{3/2}},$$ where $$v = {\\sum (x_i - \\bar{x})^2 \\over (n - 1)}.$$ Note that the skewness for a normal distribution is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell segmentation data contain a predictor that measures the standard deviation of the intensity of the pixels in the actin filaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "ax1.hist(cell_train['VarIntenCh3'].values, bins=20)\n",
    "ax1.set_xlabel('Natural Units')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.hist(np.log(cell_train['VarIntenCh3'].values), bins=20)\n",
    "ax2.set_xlabel('Log Units')\n",
    "\n",
    "ax3.hist(np.sqrt(cell_train['VarIntenCh3'].values), bins=20)\n",
    "ax3.set_xlabel('Square Root Units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows a strong right skewness. The log transformation seems to work well for this dataset. The ratio of the smallest to largest value and the sample skewness statistic all agree with the histogram under natural units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "r = np.max(cell_train['VarIntenCh3'].values)/np.min(cell_train['VarIntenCh3'].values)\n",
    "skewness = skew(cell_train['VarIntenCh3'].values)\n",
    "\n",
    "print ('Ratio of the smallest to largest value is {0} \\nSample skewness statistic is {1}'.format(r, skewness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, statistical models can be used to empirically identify an appropriate transformation. One of the most famous transformations is the Box-Cox family, i.e.\n",
    "\\begin{equation}\n",
    "x^* = \\begin{cases} {x^{\\lambda}-1 \\over \\lambda} & \\text{if} \\ \\lambda \\neq 0 \\\\ log(x) & \\text{if} \\ \\lambda = 0 \\end{cases}\n",
    "\\end{equation}\n",
    "This family covers the log ($\\lambda = 0$), square ($\\lambda = 2$), square root ($\\lambda = 0.5$), inverse ($\\lambda = -1$), and others in-between. Using the training data, $\\lambda$ can be estimated using maximum likelihood estimation (MLE). This procedure would be applied independently to each predictor data that contain values **greater than 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxcox() in *scipy.stats* finds the estimated lambda and performs the transformation at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "print ('Estimated lambda is {0}'.format(boxcox(cell_train['VarIntenCh3'].values)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another predictor for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.hist(cell_train['PerimCh1'].values, bins=20)\n",
    "ax1.set_xlabel('Natural Units')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.hist(boxcox(cell_train['PerimCh1'].values)[0], bins=20)\n",
    "ax2.set_xlabel('Transformed Data (lambda = {:1.4f})'.format(boxcox(cell_train['PerimCh1'].values)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Transformations for Multiple Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations act on groups of predictors, typically the entire set under consideration. Of primary importance are methods to resolve outliers and reduce the dimension of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations to Resolve Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally define outliers as samples that are exceptionally far from the mainstream of the data. Even with a thorough understanding of the data, outliers can be hard to define. However, we can often identify an unusual value by looking at a figure. When one or more samples are suspected to be outliers, the first step is to make sure that the values are scientifically valid and that no data recording errors have occured. Great care should be taken not to hastily remove or change values, especially if the sample size is small. With small sample sizes, apparent outliers might be a result of a skewed distribution where there are not yet enough data to see the skewness. Also, the outlying data may be an indication of a special part of the population under study that is just starting to be sampled. Depending on how the data were collected, a \"cluster\" of valid points that reside outside the mainstream of the data might belong to a different population than the other samples, e.g. *extrapolation* and *applicability domain*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several predictive models that are resistant to outliers, e.g.\n",
    "- Tree based classification models: create splits of the training set.\n",
    "- Support Vector Machines (SVM) for classification: disregard a portion if the training set that may be far away from the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model is considered to be sensitive to outliers, one data transformation that can minimize the problem is the *spatial sign*. Mathematically, each sample is divided by its squared norm: $$x_{ij}^* = {x_{ij} \\over \\sqrt{\\sum_{j=1}^p x_{ij}^2}}.$$ Since the denominator is intended to measure the squared distance to the center of the predictor's distribution, it is **important** to center and scale the predictor data prior to using this transformation. Note that, unlike centering and scaling, this manipulation of the predictors transform them as a group. Removing predictor variables after applying the spatial sign transformation may be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "beta0 = -2.3 # intercept\n",
    "beta1 = 0.8 # slope\n",
    "n = 1000\n",
    "x1_true = np.random.normal(4, 2, n)\n",
    "x2_true = np.zeros(n)\n",
    "\n",
    "# generate a random sample\n",
    "for i in range(n):\n",
    "    x2_true[i] = beta0 + beta1*x1_true[i] + np.random.normal(size = 1)\n",
    "    \n",
    "# generate outliers\n",
    "x1_outliers = np.random.uniform(-4, -3, 8)\n",
    "x2_outliers = np.zeros(8)\n",
    "for i in range(8):\n",
    "    x2_outliers[i] = x1_outliers[i] + np.random.normal(size = 1)\n",
    "\n",
    "plt.scatter(x1_true, x2_true)\n",
    "plt.plot(x1_outliers, x2_outliers, 'ro', markersize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "x1 = scale(np.concatenate([x1_true, x1_outliers]))\n",
    "x2 = scale(np.concatenate([x2_true, x2_outliers]))\n",
    "x = np.array(list(zip(x1, x2)))\n",
    "\n",
    "# spatial sign\n",
    "dist = x[:, 0]**2 + x[:, 1]**2\n",
    "x1 = x[:, 0]/np.sqrt(dist)\n",
    "x2 = x[:, 1]/np.sqrt(dist)\n",
    "\n",
    "plt.scatter(x1[:-8], x2[:-8])\n",
    "plt.plot(x1[-7:], x2[-7:], 'ro', markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *spatial sign* transformation brings the outliers towards the majority of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction and Feature Extraction\n",
    "\n",
    "Dimension Reduction: creating features by combining existing features\n",
    "\n",
    "Feature Selection (signal extraction): reducing the feature set to the most relevant features\n",
    "\n",
    "These methods reduce the data by generating a smaller set of predictors that seek to capture a majority of the information in the original variables. For most data reduction techniques, the new predictors are functions of the original predictors; therefore, all the original predictors are still needed to create the surrogate variables.\n",
    "\n",
    "Why?\n",
    "    - memory savings\n",
    "    - computational savings\n",
    "    - eliminating correlated features\n",
    "    - easier to visualize\n",
    "\n",
    "Common Dimensionality Reduction Techniques:\n",
    "\n",
    "    1. Missing Value Ratio\n",
    "    2. Low Variance Filter\n",
    "    3. High Correlation Filter\n",
    "    4. Random Forest\n",
    "    5. Backward Feature Elimination\n",
    "    6. Forward Feature Selection\n",
    "    7. Factor Analysis\n",
    "    8. Principal Component Analysis\n",
    "    9. Independent Component Analysis\n",
    "    10. Methods Based on Projections\n",
    "    11. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    12. UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Removing Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are potential advantages to removing predictors prior to modeling. First, fewer predictors means decreased computational time and complexity. Second, if two predictors are highly correlated, this implies that they are measuring the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions, e.g. near-zero variance predictors. In these cases, there can be a significant improvement in model performance and/or stability without the problematic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule of thumb for detecting near-zero variance predictors:\n",
    "- The fraction of unique values over the sample size is low (say 10%)\n",
    "- The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say around 20)\n",
    "\n",
    "If both of these criteria are true and the model in question is susceptible to this type of predictor, it may be advantageous to remove the variable from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between-Predictor Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Collinearity* is the technical term for the situation where a pair of predictor variables have a substantial correlation with each other. It is also possible to have relationships between multiple predictors at once (called *multicollinearity*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are good reasons to avoid data with highly correlated predictors. First, redundant predictors frequently add more complexity to the model than information they provide to the model. In situations where obtaining the predictor data is costly, fewer variables is obviously better. Using highly correlated predictors in techniques like linear regression can result in highly unstable models, numerical values, and degraded predictive performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical regression analysis has several tools to diagnose multicollinearity for linear regression. A statistic called the variance inflation factor (VIF) can be used to identify predictors that are impacted. A common rule of thumb is that if VIF > 5, then multicollinearity is high. Note that this method is developed for linear models, it requires more samples than predictor variables and it does not determine which should be removed to resolve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more heuristic approach is to remove the minimum number of predictors to ensure that all pairwise correlation are below a certain threshold. The algorithm is as follows:\n",
    "- Calculate the correlation matrix of the predictors.\n",
    "- Determine the two predictors associated with the largest absolute pairwise correlation (A and B).\n",
    "- Determine the average absolute correlation between A and the other variables. Do the same for predictor B.\n",
    "- If A has a larger average correlation, remove it; otherwise, remove predictor B.\n",
    "- Repeat Steps 2-4 until no absolute correlations are above the threshold.\n",
    "\n",
    "Suppose we wanted to use a model that is particularly sensitive to between predictor correlations, we might apply a threshold of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, feature extraction methods (e.g., principal components) are another technique for mitigating the effect of strong correlations between predictors. However, these techniques make the connection between the predictors and the outcome more complex. Additionally, since signal extraction methods are usually unsupervised, there is no guarantee that the resulting surrogate preditors have any relationship with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
